{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root='datasets/mnist/',transform=ToTensor(),download=True)\n",
    "test_dataset  = MNIST(root='datasets/mnist/',train=False,transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "test_loader  = DataLoader(dataset=train_dataset, batch_size=batch_size*2, shuffle=False,pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2 #2-layer stacked LSTM\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers,num_classes):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True) #batch_first for shape (batch,seq_no,features)\n",
    "        self.fc = nn.Linear(hidden_size*2,num_classes) #*2 as bidirectional\n",
    "    \n",
    "    def forward(self,X_batch):\n",
    "        hidden_0 = torch.zeros(self.num_layers*2, X_batch.size(0), self.hidden_size).to(device)\n",
    "        cellstate_0 = torch.zeros(self.num_layers*2, X_batch.size(0),self.hidden_size).to(device)\n",
    "        \n",
    "        output,_ = self.lstm(X_batch, (hidden_0,cellstate_0))\n",
    "        #output shape is (batch_size,seq_no,hidden_size)\n",
    "        # we need hidden_state of the last element in the sequence for each of the batch inputs\n",
    "        output = self.fc(output[:,-1,:])\n",
    "        return output\n",
    "    \n",
    "    def train_step(self,X_batch):\n",
    "        images,labels = X_batch\n",
    "        images = images.reshape(-1,sequence_length,input_size)\n",
    "        output = model(images)\n",
    "        loss = F.cross_entropy(output,labels)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self,X_batch):\n",
    "        images,labels = X_batch\n",
    "        images = images.reshape(-1,sequence_length,input_size)\n",
    "        output = model(images)\n",
    "        loss = F.cross_entropy(output,labels)\n",
    "        _,preds = torch.max(output,dim=1)\n",
    "        acc = torch.tensor(torch.sum(preds==labels).item()/len(preds))\n",
    "        return {'loss':loss,'acc':acc}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data,device):\n",
    "    if isinstance(data,(list,tuple)):\n",
    "        return [to_device(x,device) for x in data]\n",
    "    else:\n",
    "        return data.to(device,non_blocking=True)\n",
    "    \n",
    "class DeviceDataLoader():\n",
    "    #Wrap dataloader to move data to device\n",
    "    def __init__(self,dl,device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            yield to_device(batch,self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiRNN(\n",
       "  (lstm): LSTM(28, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiRNN(input_size, hidden_size, num_layers, num_classes)\n",
    "to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DeviceDataLoader(train_loader,device)\n",
    "test_loader = DeviceDataLoader(test_loader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model,test_loader):\n",
    "    outputs = [model.test_step(batch) for batch in test_loader]\n",
    "    batch_losses = [x['loss'] for x in outputs]\n",
    "    batch_accs = [x['acc'] for x in outputs]\n",
    "    loss = torch.stack(batch_losses).mean()\n",
    "    acc = torch.stack(batch_accs).mean()\n",
    "    print(f\"loss is {loss} and acc is {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 2.3031859397888184 and acc is 0.10253333300352097\n"
     ]
    }
   ],
   "source": [
    "evaluate(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,test_loader,num_epochs=2,lr=0.01,optim_func=torch.optim.Adam):\n",
    "    optim = optim_func(model.parameters(),lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            loss = model.train_step(batch)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        evaluate(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.11455697566270828 and acc is 0.9658666849136353\n",
      "loss is 0.08012564480304718 and acc is 0.9757999777793884\n"
     ]
    }
   ],
   "source": [
    "train(model,train_loader,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
